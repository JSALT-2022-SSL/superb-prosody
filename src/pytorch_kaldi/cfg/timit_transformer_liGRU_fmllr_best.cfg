[cfg_proto]
cfg_proto = proto/global.proto
cfg_proto_chunk = proto/global_chunk.proto

[exp]
cmd = 
run_nn_script = run_nn
out_folder = exp/timit-liGRU-BASE-960-1000
seed = 1337
use_cuda = True
multi_gpu = True
save_gpumem = False
n_epochs_tr = 32

[dataset1]
data_name = TIMIT_tr
fea = fea_name=fmllr
	fea_lst=/media/andi611/1TBSSD/kaldi/egs/timit/s5/data-fmllr-tri3/train/feats.scp
	fea_opts=apply-cmvn --utt2spk=ark:/media/andi611/1TBSSD/kaldi/egs/timit/s5/data-fmllr-tri3/train/utt2spk  ark:/media/andi611/1TBSSD/kaldi/egs/timit/s5/data-fmllr-tri3/train/cmvn_train.ark ark:- ark:- | add-deltas --delta-order=0 ark:- ark:- |
	cw_left=0
	cw_right=0
	
lab = lab_name=lab_cd
	lab_folder=/media/andi611/1TBSSD/kaldi/egs/timit/s5/exp/dnn4_pretrain-dbn_dnn_ali
	lab_opts=ali-to-pdf
	lab_count_file=auto
	lab_data_folder=/media/andi611/1TBSSD/kaldi/egs/timit/s5/data/train/
	lab_graph=/media/andi611/1TBSSD/kaldi/egs/timit/s5/exp/tri3/graph
	
	lab_name=lab_mono
	lab_folder=/media/andi611/1TBSSD/kaldi/egs/timit/s5/exp/dnn4_pretrain-dbn_dnn_ali
	lab_opts=ali-to-phones --per-frame=true
	lab_count_file=none
	lab_data_folder=/media/andi611/1TBSSD/kaldi/egs/timit/s5/data/train/
	lab_graph=/media/andi611/1TBSSD/kaldi/egs/timit/s5/exp/tri3/graph

n_chunks = 5

[dataset2]
data_name = TIMIT_dev
fea = fea_name=fmllr
	fea_lst=/media/andi611/1TBSSD/kaldi/egs/timit/s5/data-fmllr-tri3/dev/feats.scp
	fea_opts=apply-cmvn --utt2spk=ark:/media/andi611/1TBSSD/kaldi/egs/timit/s5/data-fmllr-tri3/dev/utt2spk  ark:/media/andi611/1TBSSD/kaldi/egs/timit/s5/data-fmllr-tri3/dev/cmvn_dev.ark ark:- ark:- | add-deltas --delta-order=0 ark:- ark:- |
	cw_left=0
	cw_right=0
	
lab = lab_name=lab_cd
	lab_folder=/media/andi611/1TBSSD/kaldi/egs/timit/s5/exp/dnn4_pretrain-dbn_dnn_ali_dev
	lab_opts=ali-to-pdf
	lab_count_file=auto
	lab_data_folder=/media/andi611/1TBSSD/kaldi/egs/timit/s5/data/dev/
	lab_graph=/media/andi611/1TBSSD/kaldi/egs/timit/s5/exp/tri3/graph
	
	lab_name=lab_mono
	lab_folder=/media/andi611/1TBSSD/kaldi/egs/timit/s5/exp/dnn4_pretrain-dbn_dnn_ali_dev
	lab_opts=ali-to-phones --per-frame=true
	lab_count_file=none
	lab_data_folder=/media/andi611/1TBSSD/kaldi/egs/timit/s5/data/dev/
	lab_graph=/media/andi611/1TBSSD/kaldi/egs/timit/s5/exp/tri3/graph

n_chunks = 1

[dataset3]
data_name = TIMIT_test
fea = fea_name=fmllr
	fea_lst=/media/andi611/1TBSSD/kaldi/egs/timit/s5/data-fmllr-tri3/test/feats.scp
	fea_opts=apply-cmvn --utt2spk=ark:/media/andi611/1TBSSD/kaldi/egs/timit/s5/data-fmllr-tri3/test/utt2spk  ark:/media/andi611/1TBSSD/kaldi/egs/timit/s5/data-fmllr-tri3/test/cmvn_test.ark ark:- ark:- | add-deltas --delta-order=0 ark:- ark:- |
	cw_left=0
	cw_right=0
	
lab = lab_name=lab_cd
	lab_folder=/media/andi611/1TBSSD/kaldi/egs/timit/s5/exp/dnn4_pretrain-dbn_dnn_ali_test
	lab_opts=ali-to-pdf
	lab_count_file=auto
	lab_data_folder=/media/andi611/1TBSSD/kaldi/egs/timit/s5/data/test/
	lab_graph=/media/andi611/1TBSSD/kaldi/egs/timit/s5/exp/tri3/graph
	
	lab_name=lab_mono
	lab_folder=/media/andi611/1TBSSD/kaldi/egs/timit/s5/exp/dnn4_pretrain-dbn_dnn_ali_test
	lab_opts=ali-to-phones --per-frame=true
	lab_count_file=none
	lab_data_folder=/media/andi611/1TBSSD/kaldi/egs/timit/s5/data/test/
	lab_graph=/media/andi611/1TBSSD/kaldi/egs/timit/s5/exp/tri3/graph
n_chunks = 1

[data_use]
train_with = TIMIT_tr
valid_with = TIMIT_dev
forward_with = TIMIT_test

[batches]
batch_size_train = 8
max_seq_length_train = 1000
increase_seq_length_train = True
start_seq_len_train = 100
multply_factor_seq_len_train = 2
batch_size_valid = 8
max_seq_length_valid = 1000

[architecture1]
arch_name = lin
arch_proto = proto/Lin.proto
arch_library = nn_mockingjay
arch_class = LIN
arch_pretrain_file = none
arch_freeze = False
arch_seq_model = False

arch_lr = 0.0004
arch_halving_factor = 0.5
arch_improvement_threshold = 0.001
arch_opt = rmsprop
opt_momentum = 0.0
opt_alpha = 0.95
opt_eps = 1e-8
opt_centered = False
opt_weight_decay = 0.0

[architecture2]
arch_name=TRANSFORMER_AM
arch_proto=proto/Transformer.proto
arch_library=nn_transformer
arch_class=TRANSFORMER
arch_pretrain_file = none
arch_freeze = True
arch_seq_model = True

# Transformer.proto settings
ckpt_file = /media/andi611/1TBSSD/S3PRL/result/result_transformer/tera/fmllrBase960-K-libri/states-1000000.ckpt
load_pretrain = True
no_grad = True
dropout = default
spec_aug = False
spec_aug_prev = True
weighted_sum = False
select_layer = -1
permute_input = True

# Optimizer Settings
arch_lr = 0.0000
arch_halving_factor = 0.5
arch_improvement_threshold = 0.001
arch_opt = rmsprop
opt_momentum = 0.0
opt_alpha = 0.95
opt_eps = 1e-8
opt_centered = False
opt_weight_decay = 0.0

[architecture3]
arch_name = MLP_layers_in
arch_proto = proto/MLP.proto
arch_library = neural_networks
arch_class = MLP
arch_pretrain_file = none
arch_freeze = False
arch_seq_model = False
dnn_lay = 640
dnn_drop = 0.2
dnn_use_laynorm_inp = False
dnn_use_batchnorm_inp = False
dnn_use_batchnorm = True
dnn_use_laynorm = False
dnn_act = leaky_relu
arch_lr = 0.0004
arch_halving_factor = 0.5
arch_improvement_threshold = 0.001
arch_opt = rmsprop
opt_momentum = 0.0
opt_alpha = 0.95
opt_eps = 1e-8
opt_centered = False
opt_weight_decay = 0.0

[architecture4]
arch_name = liGRU_layers
arch_proto = proto/liGRU.proto
arch_library = neural_networks
arch_class = liGRU
arch_pretrain_file = none
arch_freeze = False
arch_seq_model = True
ligru_lay = 550,550,550,550,550
ligru_drop = 0.3,0.3,0.3,0.3,0.3
ligru_use_laynorm_inp = False
ligru_use_batchnorm_inp = False
ligru_use_laynorm = False,False,False,False,False
ligru_use_batchnorm = True,True,True,True,True
ligru_bidir = True
ligru_act = relu,relu,relu,relu,relu
ligru_orthinit = True

arch_lr = 0.0004
arch_halving_factor = 0.5
arch_improvement_threshold = 0.001
arch_opt = rmsprop
opt_momentum = 0.0
opt_alpha = 0.95
opt_eps = 1e-8
opt_centered = False
opt_weight_decay = 0.0

[architecture5]
arch_name = MLP_layers1
arch_proto = proto/MLP.proto
arch_library = neural_networks
arch_class = MLP
arch_pretrain_file = none
arch_freeze = False
arch_seq_model = False
dnn_lay = N_out_lab_cd
dnn_drop = 0.0
dnn_use_laynorm_inp = False
dnn_use_batchnorm_inp = False
dnn_use_batchnorm = False
dnn_use_laynorm = False
dnn_act = softmax

# Optimizer Settings
arch_lr = 0.0004
arch_halving_factor = 0.5
arch_improvement_threshold = 0.001
arch_opt = rmsprop
opt_momentum = 0.0
opt_alpha = 0.95
opt_eps = 1e-8
opt_centered = False
opt_weight_decay = 0.0

[architecture6]
arch_name = MLP_layers2
arch_proto = proto/MLP.proto
arch_library = neural_networks
arch_class = MLP
arch_pretrain_file = none
arch_freeze = False
arch_seq_model = False
dnn_lay = N_out_lab_mono
dnn_drop = 0.0
dnn_use_laynorm_inp = False
dnn_use_batchnorm_inp = False
dnn_use_batchnorm = False
dnn_use_laynorm = False
dnn_act = softmax

# Optimizer Settings
arch_lr = 0.0004
arch_halving_factor = 0.5
arch_improvement_threshold = 0.001
arch_opt = rmsprop
opt_momentum = 0.0
opt_alpha = 0.95
opt_eps = 1e-8
opt_centered = False
opt_weight_decay = 0.0

[model]
model_proto = proto/model.proto
model = fea_tran=compute(lin,fmllr)
	fea_tran=compute(TRANSFORMER_AM,fea_tran)
	fea_tran=compute(MLP_layers_in,fea_tran)
	out_dnn1=compute(liGRU_layers,fea_tran)
	out_dnn2=compute(MLP_layers1,out_dnn1)
	out_dnn3=compute(MLP_layers2,out_dnn1)
	loss_mono=cost_nll(out_dnn3,lab_mono)
	loss_mono_w=mult_constant(loss_mono,1.0)
	loss_cd=cost_nll(out_dnn2,lab_cd)
	loss_final=sum(loss_cd,loss_mono_w)
	err_final=cost_err(out_dnn2,lab_cd)

[forward]
forward_out = out_dnn2
normalize_posteriors = True
normalize_with_counts_from = lab_cd
save_out_file = False
require_decoding = True

[decoding]
decoding_script_folder = kaldi_decoding_scripts/
decoding_script = decode_dnn.sh
decoding_proto = proto/decoding.proto
min_active = 200
max_active = 7000
max_mem = 50000000
beam = 13.0
latbeam = 8.0
acwt = 0.2
max_arcs = -1
skip_scoring = false
scoring_script = local/score.sh
scoring_opts = "--min-lmwt 1 --max-lmwt 10"
norm_vars = False

